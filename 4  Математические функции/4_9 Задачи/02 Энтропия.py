"""
Дана формула информационной энтропии:

H(x)=−∑i=1npi∗log2(pi)
где
H(x) - энтропия
pi - вероятность i-того события

Примечание: Информационная энтропия применяется в машинном обучении при построении деревьев.
Впервые понятие ввел Клод Шеннон в статье "Математическая теория связи", вот здесь можно почитать перевод (стр. 243)

Дан массив вероятностей каждого события, вычислите для него энтропию.
Ответ округлите с помощью np.around() до трех знаков после запятой
"""
import numpy as np

p = np.array([0.36, 0.013, 0.02, 0.335, 0.12, 0.007, 0.08, 0.03, 0.026, 0.009])

res = -np.sum(p * np.log2(p))
print(np.around(res, 3))
